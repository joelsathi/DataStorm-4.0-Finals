{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the necessary Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the preprocessed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = pd.read_csv(\"preprocessed_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient_ID</th>\n",
       "      <th>Pregncies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.738942</td>\n",
       "      <td>0.841697</td>\n",
       "      <td>0.079257</td>\n",
       "      <td>0.878673</td>\n",
       "      <td>-1.015223</td>\n",
       "      <td>0.174836</td>\n",
       "      <td>0.823401</td>\n",
       "      <td>1.362184</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.815412</td>\n",
       "      <td>-1.101194</td>\n",
       "      <td>-0.293974</td>\n",
       "      <td>0.633136</td>\n",
       "      <td>-1.015223</td>\n",
       "      <td>-0.726206</td>\n",
       "      <td>-0.168910</td>\n",
       "      <td>0.124186</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.199867</td>\n",
       "      <td>1.983640</td>\n",
       "      <td>-0.414110</td>\n",
       "      <td>-1.432077</td>\n",
       "      <td>-1.015223</td>\n",
       "      <td>-1.130359</td>\n",
       "      <td>0.937118</td>\n",
       "      <td>0.227845</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.815412</td>\n",
       "      <td>-0.983143</td>\n",
       "      <td>-0.293974</td>\n",
       "      <td>0.362290</td>\n",
       "      <td>0.849231</td>\n",
       "      <td>-0.537930</td>\n",
       "      <td>-1.302353</td>\n",
       "      <td>-1.480337</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-2.025613</td>\n",
       "      <td>0.491042</td>\n",
       "      <td>-1.676657</td>\n",
       "      <td>0.878673</td>\n",
       "      <td>1.069805</td>\n",
       "      <td>1.479798</td>\n",
       "      <td>2.339558</td>\n",
       "      <td>0.324975</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Patient_ID  Pregncies   Glucose  BloodPressure  SkinThickness   Insulin   \n",
       "0           1   0.738942  0.841697       0.079257       0.878673 -1.015223  \\\n",
       "1           2  -0.815412 -1.101194      -0.293974       0.633136 -1.015223   \n",
       "2           3   1.199867  1.983640      -0.414110      -1.432077 -1.015223   \n",
       "3           4  -0.815412 -0.983143      -0.293974       0.362290  0.849231   \n",
       "4           5  -2.025613  0.491042      -1.676657       0.878673  1.069805   \n",
       "\n",
       "        BMI  DiabetesPedigreeFunction       Age  Outcome  \n",
       "0  0.174836                  0.823401  1.362184        1  \n",
       "1 -0.726206                 -0.168910  0.124186        0  \n",
       "2 -1.130359                  0.937118  0.227845        1  \n",
       "3 -0.537930                 -1.302353 -1.480337        0  \n",
       "4  1.479798                  2.339558  0.324975        1  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Patient_ID                    int64\n",
       "Pregncies                   float64\n",
       "Glucose                     float64\n",
       "BloodPressure               float64\n",
       "SkinThickness               float64\n",
       "Insulin                     float64\n",
       "BMI                         float64\n",
       "DiabetesPedigreeFunction    float64\n",
       "Age                         float64\n",
       "Outcome                       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select the best features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_eval_scores(y_pred_train, y_pred_dev, y_train, y_dev):\n",
    "    accuracy_score_val_train = accuracy_score(y_pred_train, y_train)\n",
    "    accuracy_score_val_dev = accuracy_score(y_pred_dev, y_dev)\n",
    "\n",
    "    print(\"Accuracy score on train set: \", accuracy_score_val_train)\n",
    "    print(\"Accuracy score on dev set: \", accuracy_score_val_dev)\n",
    "\n",
    "    recall_score_val_train = recall_score(y_pred_train, y_train)\n",
    "    recall_score_val_dev = recall_score(y_pred_dev, y_dev)\n",
    "\n",
    "    print(\"Recall score on train set: \", recall_score_val_train)\n",
    "    print(\"Recall score on dev set: \", recall_score_val_dev)\n",
    "\n",
    "    precision_score_val_train = precision_score(y_pred_train, y_train)\n",
    "    precision_score_val_dev = precision_score(y_pred_dev, y_dev)\n",
    "\n",
    "    print(\"Precision score on train set: \", precision_score_val_train)\n",
    "    print(\"Precision score on dev set: \", precision_score_val_dev)\n",
    "\n",
    "    f1_score_val_train = f1_score(y_pred_train, y_train)\n",
    "    f1_score_val_dev = f1_score(y_pred_dev, y_dev)\n",
    "\n",
    "    print(\"F1 score on train set: \", f1_score_val_train)\n",
    "    print(\"F1 score on dev set: \", f1_score_val_dev)\n",
    "\n",
    "    confusion_matrix_val_train = confusion_matrix(y_pred_train, y_train)\n",
    "    confusion_matrix_val_dev = confusion_matrix(y_pred_dev, y_dev)\n",
    "\n",
    "    print(\"Confusion matrix on train set:\")\n",
    "    print(confusion_matrix_val_train)\n",
    "    print()\n",
    "\n",
    "    print(\"Confusion matrix on dev set:\")\n",
    "    print(confusion_matrix_val_dev)\n",
    "    print()\n",
    "\n",
    "    classification_report_val_train = classification_report(y_pred_train, y_train)\n",
    "    classification_report_val_dev = classification_report(y_pred_dev, y_dev)\n",
    "\n",
    "    print(\"Classification report on train set:\")\n",
    "    print(classification_report_val_train)\n",
    "    print()\n",
    "\n",
    "    print(\"Classification report on dev set:\")\n",
    "    print(classification_report_val_dev)\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data into train and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((492, 8), (124, 8), (154, 8))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = preprocessed_data.drop([\"Patient_ID\", \"Outcome\"], axis=1)\n",
    "y = preprocessed_data[\"Outcome\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# split it into train and dev set\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size=0.2)\n",
    "\n",
    "# check the shape of the train, dev and test set\n",
    "X_train.shape, X_dev.shape, X_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameter tuning\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can use the following for the scoring**\n",
    "\n",
    "accuracy: The fraction of predictions that are correct.\n",
    "\n",
    "balanced_accuracy: The average of the recall and precision scores.\n",
    "\n",
    "f1_score: The harmonic mean of the recall and precision scores.\n",
    "\n",
    "log_loss: The cross-entropy loss.\n",
    "\n",
    "mean_squared_error: The mean squared error.\n",
    "\n",
    "mean_absolute_error: The mean absolute error.\n",
    "\n",
    "mean_absolute_percentage_error: The mean absolute percentage error.\n",
    "\n",
    "r2: The coefficient of determination.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If we want to use any other scoring method, we have to implement the function ourselves.**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search by GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "def tune_by_GridSearchCV(\n",
    "    model, param_grid, X_train, y_train, cross_val_folds=5, scoring_method=\"accuracy\"\n",
    "):\n",
    "    grid_search = GridSearchCV(\n",
    "        model,\n",
    "        param_grid,\n",
    "        cv=cross_val_folds,\n",
    "        scoring=scoring_method,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters: \", grid_search.best_params_)\n",
    "    print(\"Best score: \", grid_search.best_score_)\n",
    "\n",
    "    # print()\n",
    "    # print(\"The results of the Grid Search CV:\")\n",
    "    # print(grid_search.cv_results_)\n",
    "    # print()\n",
    "\n",
    "    # Returns the best predicted model\n",
    "    return grid_search.best_estimator_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search by RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "def tune_by_RandomizedSearchCV(\n",
    "    model, param_grid, X_train, y_train, cross_val_folds=5, scoring_method=\"accuracy\"\n",
    "):\n",
    "    random_search = RandomizedSearchCV(\n",
    "        model,\n",
    "        param_grid,\n",
    "        cv=cross_val_folds,\n",
    "        scoring=scoring_method,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters: \", random_search.best_params_)\n",
    "    print(\"Best score: \", random_search.best_score_)\n",
    "\n",
    "    # print()\n",
    "    # print(\"The results of the Randomized Search CV:\")\n",
    "    # print(random_search.cv_results_)\n",
    "    # print()\n",
    "\n",
    "    # Returns the best predicted model\n",
    "    return random_search.best_estimator_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. KNN Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on train set:  0.8109756097560976\n",
      "Accuracy score on dev set:  0.75\n",
      "Recall score on train set:  0.7666666666666667\n",
      "Recall score on dev set:  0.6923076923076923\n",
      "Precision score on train set:  0.6647398843930635\n",
      "Precision score on dev set:  0.43902439024390244\n",
      "F1 score on train set:  0.7120743034055728\n",
      "F1 score on dev set:  0.5373134328358209\n",
      "Confusion matrix on train set:\n",
      "[[284  58]\n",
      " [ 35 115]]\n",
      "\n",
      "Confusion matrix on dev set:\n",
      "[[75 23]\n",
      " [ 8 18]]\n",
      "\n",
      "Classification report on train set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.83      0.86       342\n",
      "           1       0.66      0.77      0.71       150\n",
      "\n",
      "    accuracy                           0.81       492\n",
      "   macro avg       0.78      0.80      0.79       492\n",
      "weighted avg       0.82      0.81      0.81       492\n",
      "\n",
      "\n",
      "Classification report on dev set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.77      0.83        98\n",
      "           1       0.44      0.69      0.54        26\n",
      "\n",
      "    accuracy                           0.75       124\n",
      "   macro avg       0.67      0.73      0.68       124\n",
      "weighted avg       0.81      0.75      0.77       124\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = knn.predict(X_train)\n",
    "y_pred_dev = knn.predict(X_dev)\n",
    "\n",
    "print_eval_scores(y_pred_train, y_pred_dev, y_train, y_dev)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameter tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"n_neighbors\": np.arange(1, 100, 2),\n",
    "    \"weights\": [\"uniform\", \"distance\"],\n",
    "    \"metric\": [\"euclidean\", \"manhattan\", \"chebyshev\"],\n",
    "}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "tuned_model = tune_by_RandomizedSearchCV(knn, param_grid, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = tuned_model.predict(X_train)\n",
    "y_pred_dev = tuned_model.predict(X_dev)\n",
    "\n",
    "print_eval_scores(y_pred_train, y_pred_dev, y_train, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "\n",
    "tuned_model = tune_by_GridSearchCV(knn, param_grid, X_train, y_train)\n",
    "\n",
    "y_pred_train = tuned_model.predict(X_train)\n",
    "y_pred_dev = tuned_model.predict(X_dev)\n",
    "\n",
    "print_eval_scores(y_pred_train, y_pred_dev, y_train, y_dev)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = log_reg.predict(X_train)\n",
    "y_pred_dev = log_reg.predict(X_dev)\n",
    "\n",
    "print_eval_scores(y_pred_train, y_pred_dev, y_train, y_dev)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"penalty\": [\"l1\", \"l2\", \"elasticnet\", \"none\"],\n",
    "    \"C\": np.logspace(-4, 4, 20),\n",
    "    \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"],\n",
    "    \"max_iter\": [100, 1000, 2500, 5000],\n",
    "}\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "tuned_model = tune_by_RandomizedSearchCV(log_reg, param_grid, X_train, y_train)\n",
    "\n",
    "y_pred_train = tuned_model.predict(X_train)\n",
    "y_pred_dev = tuned_model.predict(X_dev)\n",
    "\n",
    "print_eval_scores(y_pred_train, y_pred_dev, y_train, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()\n",
    "\n",
    "tuned_model = tune_by_GridSearchCV(log_reg, param_grid, X_train, y_train)\n",
    "\n",
    "y_pred_train = tuned_model.predict(X_train)\n",
    "y_pred_dev = tuned_model.predict(X_dev)\n",
    "\n",
    "print_eval_scores(y_pred_train, y_pred_dev, y_train, y_dev)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on train set:  1.0\n",
      "Accuracy score on dev set:  0.7580645161290323\n",
      "Recall score on train set:  1.0\n",
      "Recall score on dev set:  0.7037037037037037\n",
      "Precision score on train set:  1.0\n",
      "Precision score on dev set:  0.4634146341463415\n",
      "F1 score on train set:  1.0\n",
      "F1 score on dev set:  0.5588235294117647\n",
      "Confusion matrix on train set:\n",
      "[[319   0]\n",
      " [  0 173]]\n",
      "\n",
      "Confusion matrix on dev set:\n",
      "[[75 22]\n",
      " [ 8 19]]\n",
      "\n",
      "Classification report on train set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       319\n",
      "           1       1.00      1.00      1.00       173\n",
      "\n",
      "    accuracy                           1.00       492\n",
      "   macro avg       1.00      1.00      1.00       492\n",
      "weighted avg       1.00      1.00      1.00       492\n",
      "\n",
      "\n",
      "Classification report on dev set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.77      0.83        97\n",
      "           1       0.46      0.70      0.56        27\n",
      "\n",
      "    accuracy                           0.76       124\n",
      "   macro avg       0.68      0.74      0.70       124\n",
      "weighted avg       0.81      0.76      0.77       124\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = rf.predict(X_train)\n",
    "y_pred_dev = rf.predict(X_dev)\n",
    "\n",
    "print_eval_scores(y_pred_train, y_pred_dev, y_train, y_dev)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "25 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 434, in fit\n",
      "    raise ValueError(\"Out of bag estimation only available if bootstrap=True\")\n",
      "ValueError: Out of bag estimation only available if bootstrap=True\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "d:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.75992579        nan        nan 0.67476809\n",
      "        nan 0.74588745 0.77425273 0.7723356 ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'warm_start': False, 'oob_score': False, 'n_estimators': 1000, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'max_depth': 60, 'criterion': 'entropy', 'class_weight': None, 'bootstrap': False}\n",
      "Best score:  0.7742527313955886\n",
      "Accuracy score on train set:  0.959349593495935\n",
      "Accuracy score on dev set:  0.7419354838709677\n",
      "Recall score on train set:  0.9580838323353293\n",
      "Recall score on dev set:  0.6666666666666666\n",
      "Precision score on train set:  0.9248554913294798\n",
      "Precision score on dev set:  0.43902439024390244\n",
      "F1 score on train set:  0.9411764705882353\n",
      "F1 score on dev set:  0.5294117647058824\n",
      "Confusion matrix on train set:\n",
      "[[312  13]\n",
      " [  7 160]]\n",
      "\n",
      "Confusion matrix on dev set:\n",
      "[[74 23]\n",
      " [ 9 18]]\n",
      "\n",
      "Classification report on train set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97       325\n",
      "           1       0.92      0.96      0.94       167\n",
      "\n",
      "    accuracy                           0.96       492\n",
      "   macro avg       0.95      0.96      0.96       492\n",
      "weighted avg       0.96      0.96      0.96       492\n",
      "\n",
      "\n",
      "Classification report on dev set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.76      0.82        97\n",
      "           1       0.44      0.67      0.53        27\n",
      "\n",
      "    accuracy                           0.74       124\n",
      "   macro avg       0.67      0.71      0.68       124\n",
      "weighted avg       0.79      0.74      0.76       124\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200, 500, 1000],\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"max_depth\": [i for i in range(3, 100)],\n",
    "    \"min_samples_split\": [2, 5, 10, 20, 50, 100],\n",
    "    \"min_samples_leaf\": [1, 2, 5, 10, 20, 50, 100],\n",
    "    \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "    \"bootstrap\": [True, False],\n",
    "    \"oob_score\": [True, False],\n",
    "    \"warm_start\": [True, False],\n",
    "    \"class_weight\": [\"balanced\", \"balanced_subsample\", None],\n",
    "}\n",
    "\n",
    "tuned_model = tune_by_RandomizedSearchCV(rf, param_grid, X_train, y_train)\n",
    "\n",
    "y_pred_train = tuned_model.predict(X_train)\n",
    "y_pred_dev = tuned_model.predict(X_dev)\n",
    "\n",
    "print_eval_scores(y_pred_train, y_pred_dev, y_train, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X_train.columns.to_list()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the decision tree\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will select a tree to visualize, and set the maximum depth of the tree to 3 levels.\n",
    "\n",
    "The number of estimators we pass in the RandomForestClassifier is the number of trees in the forest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import tree\n",
    "# import matplotlib.pyplot as plt\n",
    "# fig, _ = plt.subplots(nrows=1, ncols=1, figsize=(100,50), dpi=300)\n",
    "# decision_tree = tuned_model.estimators_[0]\n",
    "# tree.plot_tree(\n",
    "#     decision_tree,\n",
    "#     feature_names=feature_names,\n",
    "#     filled=True\n",
    "# )\n",
    "# fig.savefig('tree.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize with shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "(slice(None, None, None), 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32md:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32md:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\pandas\\_libs\\index.pyx:153\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '(slice(None, None, None), 0)' is an invalid key",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39m# Plot the data and the polygons\u001b[39;00m\n\u001b[0;32m     17\u001b[0m fig, ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots()\n\u001b[1;32m---> 18\u001b[0m ax\u001b[39m.\u001b[39mscatter(X_train[:, \u001b[39m0\u001b[39;49m], X_train[:, \u001b[39m1\u001b[39m], c\u001b[39m=\u001b[39my_train, cmap\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcool\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m polygon \u001b[39min\u001b[39;00m polygons:\n\u001b[0;32m     20\u001b[0m   ax\u001b[39m.\u001b[39mplot(\u001b[39m*\u001b[39mpolygon\u001b[39m.\u001b[39mexterior\u001b[39m.\u001b[39mxy, color\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mblack\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32md:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3760\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3761\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3762\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3763\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32md:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3659\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3654\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3656\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3657\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m-> 3659\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_indexing_error(key)\n\u001b[0;32m   3660\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[1;32md:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5736\u001b[0m, in \u001b[0;36mIndex._check_indexing_error\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   5732\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_indexing_error\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[0;32m   5733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_scalar(key):\n\u001b[0;32m   5734\u001b[0m         \u001b[39m# if key is not a scalar, directly raise an error (the code below\u001b[39;00m\n\u001b[0;32m   5735\u001b[0m         \u001b[39m# would convert to numpy arrays and raise later any way) - GH29926\u001b[39;00m\n\u001b[1;32m-> 5736\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n",
      "\u001b[1;31mInvalidIndexError\u001b[0m: (slice(None, None, None), 0)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from shapely.geometry import Polygon\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the feature importances\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# Create a polygon for each feature\n",
    "polygons = []\n",
    "for i in range(len(feature_names)):\n",
    "  points = feature_importances.reshape(-1, 2)\n",
    "  polygons.append(Polygon(points))\n",
    "\n",
    "# Plot the data and the polygons\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='cool')\n",
    "for polygon in polygons:\n",
    "  ax.plot(*polygon.exterior.xy, color='black')\n",
    "ax.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize with lime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "(slice(None, None, None), 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32md:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32md:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\pandas\\_libs\\index.pyx:153\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '(slice(None, None, None), 0)' is an invalid key",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m explainer \u001b[39m=\u001b[39m lime\u001b[39m.\u001b[39;49mlime_tabular\u001b[39m.\u001b[39;49mLimeTabularExplainer(\n\u001b[0;32m      2\u001b[0m     X_train, feature_names\u001b[39m=\u001b[39;49mfeature_names, class_names\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39m0\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m\"\u001b[39;49m], mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mclassification\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[0;32m      3\u001b[0m )\n\u001b[0;32m      5\u001b[0m \u001b[39m# Select an instance for interpretation\u001b[39;00m\n\u001b[0;32m      6\u001b[0m instance_index \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32md:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\lime\\lime_tabular.py:215\u001b[0m, in \u001b[0;36mLimeTabularExplainer.__init__\u001b[1;34m(self, training_data, mode, training_labels, feature_names, categorical_features, categorical_names, kernel_width, kernel, verbose, class_names, feature_selection, discretize_continuous, discretizer, sample_around_instance, random_state, training_data_stats)\u001b[0m\n\u001b[0;32m    209\u001b[0m     discretizer \u001b[39m=\u001b[39m StatsDiscretizer(training_data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcategorical_features,\n\u001b[0;32m    210\u001b[0m                                    \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_names, labels\u001b[39m=\u001b[39mtraining_labels,\n\u001b[0;32m    211\u001b[0m                                    data_stats\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_data_stats,\n\u001b[0;32m    212\u001b[0m                                    random_state\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_state)\n\u001b[0;32m    214\u001b[0m \u001b[39mif\u001b[39;00m discretizer \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mquartile\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdiscretizer \u001b[39m=\u001b[39m QuartileDiscretizer(\n\u001b[0;32m    216\u001b[0m             training_data, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcategorical_features,\n\u001b[0;32m    217\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_names, labels\u001b[39m=\u001b[39;49mtraining_labels,\n\u001b[0;32m    218\u001b[0m             random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state)\n\u001b[0;32m    219\u001b[0m \u001b[39melif\u001b[39;00m discretizer \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdecile\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    220\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdiscretizer \u001b[39m=\u001b[39m DecileDiscretizer(\n\u001b[0;32m    221\u001b[0m             training_data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcategorical_features,\n\u001b[0;32m    222\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_names, labels\u001b[39m=\u001b[39mtraining_labels,\n\u001b[0;32m    223\u001b[0m             random_state\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_state)\n",
      "File \u001b[1;32md:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\lime\\discretize.py:178\u001b[0m, in \u001b[0;36mQuartileDiscretizer.__init__\u001b[1;34m(self, data, categorical_features, feature_names, labels, random_state)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, data, categorical_features, feature_names, labels\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, random_state\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 178\u001b[0m     BaseDiscretizer\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39mself\u001b[39;49m, data, categorical_features,\n\u001b[0;32m    179\u001b[0m                              feature_names, labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m    180\u001b[0m                              random_state\u001b[39m=\u001b[39;49mrandom_state)\n",
      "File \u001b[1;32md:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\lime\\discretize.py:51\u001b[0m, in \u001b[0;36mBaseDiscretizer.__init__\u001b[1;34m(self, data, categorical_features, feature_names, labels, random_state, data_stats)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_state \u001b[39m=\u001b[39m check_random_state(random_state)\n\u001b[0;32m     50\u001b[0m \u001b[39m# To override when implementing a custom binning\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m bins \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbins(data, labels)\n\u001b[0;32m     52\u001b[0m bins \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39munique(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m bins]\n\u001b[0;32m     54\u001b[0m \u001b[39m# Read the stats from data_stats if exists\u001b[39;00m\n",
      "File \u001b[1;32md:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\lime\\discretize.py:185\u001b[0m, in \u001b[0;36mQuartileDiscretizer.bins\u001b[1;34m(self, data, labels)\u001b[0m\n\u001b[0;32m    183\u001b[0m bins \u001b[39m=\u001b[39m []\n\u001b[0;32m    184\u001b[0m \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_discretize:\n\u001b[1;32m--> 185\u001b[0m     qts \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(np\u001b[39m.\u001b[39mpercentile(data[:, feature], [\u001b[39m25\u001b[39m, \u001b[39m50\u001b[39m, \u001b[39m75\u001b[39m]))\n\u001b[0;32m    186\u001b[0m     bins\u001b[39m.\u001b[39mappend(qts)\n\u001b[0;32m    187\u001b[0m \u001b[39mreturn\u001b[39;00m bins\n",
      "File \u001b[1;32md:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3760\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3761\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3762\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3763\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32md:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3659\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3654\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3656\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3657\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m-> 3659\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_indexing_error(key)\n\u001b[0;32m   3660\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[1;32md:\\University\\Academics_2020_4th_Sem\\DS ML Templates\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5736\u001b[0m, in \u001b[0;36mIndex._check_indexing_error\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   5732\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_indexing_error\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[0;32m   5733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_scalar(key):\n\u001b[0;32m   5734\u001b[0m         \u001b[39m# if key is not a scalar, directly raise an error (the code below\u001b[39;00m\n\u001b[0;32m   5735\u001b[0m         \u001b[39m# would convert to numpy arrays and raise later any way) - GH29926\u001b[39;00m\n\u001b[1;32m-> 5736\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n",
      "\u001b[1;31mInvalidIndexError\u001b[0m: (slice(None, None, None), 0)"
     ]
    }
   ],
   "source": [
    "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    X_train, feature_names=feature_names, class_names=[\"0\", \"1\"], mode='classification'\n",
    ")\n",
    "\n",
    "# Select an instance for interpretation\n",
    "instance_index = 1\n",
    "instance = X_test[instance_index]\n",
    "true_label = y_test[instance_index]\n",
    "\n",
    "# Generate local explanation using LIME\n",
    "exp = explainer.explain_instance(instance, tuned_model.predict_proba, num_features=len(feature_names), labels=[0, 1])\n",
    "\n",
    "# Visualize feature importances\n",
    "fig = exp.as_pyplot_figure()\n",
    "plt.title(f\"Feature Importances for Instance {instance_index}\\nTrue Label: {true_label}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC()\n",
    "\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = svc.predict(X_train)\n",
    "y_pred_dev = svc.predict(X_dev)\n",
    "\n",
    "print_eval_scores(y_pred_train, y_pred_dev, y_train, y_dev)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "\n",
    "param_grid = {\n",
    "    \"C\": [i for i in range(0.1, 10, 0.1)],\n",
    "    \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "    \"degree\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    \"gamma\": [\"scale\", \"auto\"],\n",
    "    \"coef0\": [0.0, 0.1, 0.5, 1.0, 2.0],\n",
    "    \"shrinking\": [True, False],\n",
    "    \"probability\": [True, False],\n",
    "    \"tol\": [0.001, 0.01, 0.1, 1.0],\n",
    "    \"cache_size\": [200, 500, 1000],\n",
    "    \"class_weight\": [\"balanced\", None],\n",
    "}\n",
    "\n",
    "tuned_model = tune_by_RandomizedSearchCV(svc, param_grid, X_train, y_train)\n",
    "\n",
    "y_pred_train = tuned_model.predict(X_train)\n",
    "y_pred_dev = tuned_model.predict(X_dev)\n",
    "\n",
    "print_eval_scores(y_pred_train, y_pred_dev, y_train, y_dev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
